}
glsModel <- function(data_frame, variables){
check.packages('nlme')
"data <- data.frame(row.names = 1:nrow(data_frame))
colNames <- colnames(data_frame)
for(i in 1:variables){
data[colNames[i]] <- data[,i]
}"
model <- gls(data_frame[,5] ~ data_frame[,1]+data_frame[,2]+data_frame[,3]+data_frame[,4])
return(model)
}
#quarters <- as.Date(as.matrix(fairValue[,1]))
fairValue <- convertToXTS('fairValue_only.xlsx')
setwd("~/Documents/futuregrowth_research/codebase")
#quarters <- as.Date(as.matrix(fairValue[,1]))
fairValue <- convertToXTS('fairValue_only.xlsx')
##Create a dataframe to store the coefficients in
##create a dataframe tp add coefficients\
coefs <- data.frame(row.names = c('intercept', 'us_long_term', 'sa_gdp', 'sa_headline_cpi', 'sa_policy_rate'))
output <- data.frame(row.names = 1:8)
outputResidualsForGLS <- data.frame(row.names = c('R2', 'RMSE', 'MAE'))
check.packages(c('ggplot2', 'tseries', 'reshape2', 'modelr'))
"###A PROPER WAY of writing the solution
rollRegress <- function(FUN, startPos, startYear, window, tsDATA){
tslength = nrow(tsDATA) - window
years = (tslength+1)/4
year = 0
count = 0
while(startPos < (tslength+1) && year< years+1 ){
model_data = ts(ts)
}
}"
i = 101
n = 0
##count to mod by 4
count = 0
while ( i <= 104 && n <12){
startYear = 1990+n
endYear = 2017+n
window_size = i+112
#the time series obejct
ts_data1 <- ts(fairValue[i:window_size,], start= startYear, end = c(endYear, 4), frequency = 4)
##The variables
us_long = ts_data1[,1]
sa_headline_cpi = ts_data1[,2]
sa_st_policy = ts_data1[,3]
sa_dGDP = ts_data1[,4]
sa_long = ts_data1[,5]
modbind = cbind(us_long, sa_headline_cpi, sa_st_policy, sa_dGDP, sa_long)
#the gls model
GLSroll = gls(sa_long ~ us_long+sa_dGDP+sa_headline_cpi+sa_st_policy,
correlation = corAR1())
summ = summary(GLSroll)
##store coefficients to a dataframe
coefs[paste(startYear,count,sep = '.')]  <- as.data.frame(summ$coefficients)
#print(paste('startYear',paste(startYear, cunt) ,sep = ': '))
##output the predicted variables to a dataframe
glsDATA = as.data.frame(summ$coefficients)
francis <- data.frame(
R2 = rsquare(GLSroll, data=ts_data1),
RMSE = rmse(GLSroll, data=ts_data1),
MAE = mae(GLSroll, data=ts_data1)
)
outputResidualsForGLS[paste(startYear,count,sep = '.')] <- t(francis)
##update the while loop[]
i = i+1
count = count +1
if( count %% 4 == 0){
n=n+1
}
}
rm(list = ls())
graphics.off()
check.packages <- function(pkg) {
new.pkg <- pkg[! (pkg %in% installed.packages()[, 'Package'])]
if(length(new.pkg))
install.packages(new.pkg, dependencies = T)
sapply(pkg, require, character.only = T)
}
check.packages(c('dlm','vars', 'mFilter'))
fairValue <- read_excel('fairValue_only.xlsx')
check.packages <- function(pkg) {
new.pkg <- pkg[! (pkg %in% installed.packages()[, 'Package'])]
if(length(new.pkg))
install.packages(new.pkg, dependencies = T)
sapply(pkg, require, character.only = T)
}
check.packages(c('dlm','vars', 'mFilter'))
check.packages('readxl')
fairValue <- read_excel('fairValue_only.xlsx')
quarters <- as.Date(as.matrix(fairValue[,1]))
fairValue <- as.data.frame(fairValue)
#zoo (xts)
check.packages('xts')
fairValue <- xts(fairValue[,-1], order.by = as.Date.POSIXct(fairValue[,1], "%Y-%m-%d"))
##convert to time series object
##112 items start in 1987
check.packages('imputeTS')
ts_data <- ts(fairValue, start = 1965, end = c(1991,4) , frequency = 4)
ts_data <- na.seadec(ts_data, algorithm = 'interpolation')
##all variables
us_long_term = ts_data[,1]
sa_cpi = ts_data[,2]
sa_policy_rate = ts_data[,3]
sa_gdp = ts_data[,4]
sa_long_term = ts_data[,5]
##visualisa
plot.ts(cbind(us_long_term, sa_cpi, sa_policy_rate, sa_gdp, sa_long_term))
##check for serial correllation
#devtools::install_github("KevinKotze/tsm")
check.packages('tsm')
##check for serial correllation
devtools::install_github("KevinKotze/tsm")
##check for serial correllation
#devtools::install_github("KevinKotze/tsm")
check.packages('tsm')
us = ac(us_long_term, main='US long term')
cpi = ac(sa_cpi, main='Sa CPI')
policy_rate = ac(sa_policy_rate, main='Sa policy rate')
gdp.acf = ac(sa_gdp, main='GDP')
sa_long = ac(sa_long_term, main='SA long term')
check.packages(c('tseries', 'lmtest', 'orcutt'))
##unit roots
adf.test(us_long_term)
adf.test(sa_cpi)
adf.test(sa_policy_rate)
adf.test(sa_gdp)
adf.test(sa_long_term)
##first difference
##differentiate
us_long_diff = diff(us_long_term,1)
sa_cpi_diff = diff(sa_cpi,1)
sa_policy_diff = diff(sa_policy_rate,1)
sa_gdp_diff = diff(sa_gdp,1)
sa_long_diff = diff(sa_long_term,1)
check.packages('forecast')
modbind= cbind(sa_long_term, us_long_term, sa_cpi, sa_policy_rate, sa_gdp)
modstat = cbind(sa_long_diff, us_long_diff, sa_cpi_diff, sa_policy_diff, sa_gdp_diff)
##plot next to each otehr
check.packages('ggplot2')
check.packages('caret')
check.packages('np')
#modfit
modfit <- lm(sa_long_term ~ sa_cpi+sa_gdp+sa_policy_rate+us_long_term)
modfitDiff <- lm(sa_long_diff ~ sa_cpi_diff+sa_gdp_diff+sa_policy_diff+us_long_diff, data = modstat)
summary(modfitDiff)
summary(modfit)
###fitted
autoplot(modfit[,'sa_long_term'], series="Data") +
autolayer(fitted(modfitDiff), series="Fitted") +
xlab("Year") + ylab("") +
ggtitle("fair value mode") +
guides(colour=guide_legend(title=" "))
##evaluate teh residuals
checkresiduals(modfit)
checkresiduals(modfitDiff)
check.packages('dynlm')
install.packages("rJava")
setwd("~/Documents/futuregrowth_research/thabo_environment")
##then type main() in the console
if('openxlsx' %in% (.packages())){
detach('package:openxlsx', unload = T)
}
#lapply(packages, require, character.only = T)
check.packages <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c('readxl','rJava', 'xts','container', 'dint', 'vars', 'astsa', 'quantmod', 'TSclust',
'forecast', 'imputeTS', 'tseries', 'TTR', 'ggplot2', 'reshape2', 'leaps', 'pracma',
'broom', 'xlsx', 'lmtest', 'tibble', 'outreg')
check.packages(packages)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
View(xl_data)
View(xl_data[1])
View(xl_data[,1])
plot(allSheets[[1]][,2])
View(allSheets[[1]][,2])
View(allSheets[[1]])
View(var_dict[sheetNames[1]])
View(datum)
count_names
##add variables to the dictionary
for( i in 1:length(regions)){
country <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
country[freq_var] <- quarters
region <- count_dict[regions[i]]
for(j in 1:length(sheetNames)){
datum <- var_dict[sheetNames[j]]
count_names <- colnames(datum)
country[sheetNames[j]]  <- datum[regions[i]]
var_dict$set(sheetNames[j], xts(var_dict[sheetNames[i]][-1], order.by = as.Date(var_dict[sheetNames[j]][,1], '%Y-%m-%d')))
}
#nuDF <- cbind(region, country)
count_dict$set(regions[i], xts(country[-1], order.by = as.Date(country[,1], '%Y-%m-%d')))
}
##add variables to the dictionary
for( i in 1:length(regions)){
country <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
country[freq_var] <- quarters
region <- count_dict[regions[i]]
for(j in 1:length(sheetNames)){
datum <- var_dict[sheetNames[j]]
count_names <- colnames(datum)
country[sheetNames[j]]  <- datum[regions[i]]
#var_dict$set(sheetNames[j], xts(var_dict[sheetNames[i]][-1], order.by = as.Date(var_dict[sheetNames[j]][,1], '%Y-%m-%d')))
}
#nuDF <- cbind(region, country)
count_dict$set(regions[i], xts(country[-1], order.by = as.Date(country[,1], '%Y-%m-%d')))
}
##Create a dictionary for countries
count_dict <- container::dict()
for(i in 1:length(regions)){
newDF <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
newDF[freq_var] <- quarters
count_dict[regions[i]] <- newDF
}
##add variables to the dictionary
for( i in 1:length(regions)){
country <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
country[freq_var] <- quarters
region <- count_dict[regions[i]]
for(j in 1:length(sheetNames)){
datum <- var_dict[sheetNames[j]]
count_names <- colnames(datum)
country[sheetNames[j]]  <- datum[regions[i]]
#var_dict$set(sheetNames[j], xts(var_dict[sheetNames[i]][-1], order.by = as.Date(var_dict[sheetNames[j]][,1], '%Y-%m-%d')))
}
#nuDF <- cbind(region, country)
count_dict$set(regions[i], xts(country[-1], order.by = as.Date(country[,1], '%Y-%m-%d')))
}
#if(howManySheet == 'Yes')
if(howManySheet == 'Yes'){
allSheets <- lapply(excel_sheets(excel_file), read_excel, path=excel_file)
#xl_data <- loadWorkbook(excel_file)
xl_data <- xlsx::loadWorkbook(excel_file)
sheetNames <- names(getSheets(xl_data))
#get all regions
regions <- colnames(allSheets[[1]])
regions <- regions[2:length(regions)]
###convert strings to data
quarters <- as.Date(as.matrix(allSheets[[1]][,1]))
print('This a useless plot to test whether the data has been read well')
print("It is also required to initialize plot() function")
plot(allSheets[[1]][,2])
##dictionary of variables by their countries
var_dict <- container::dict()
for(i in 1:length(allSheets)){
var_dict[sheetNames[i]] <- as.data.frame(read_xlsx(excel_file, sheet = sheetNames[i]))
}
##the periodicy of the data
print("Is it quarterly or monthly or annually?")
bool_val <- F
while(bool_val != T){
freq_var <- readline("Enter whether its a 'Quarter', a 'Month' or an 'Annual' data: ")
freq_num <- 0
if(freq_var == 'Quarter'){
freq_num = 4
bool_val = T
} else if(freq_var == 'Month'){
freq_num = 12
bool_val = T
} else if(freq_var == 'Annual'){
freq_num = 1
bool_val = T
}
}
##Create a dictionary for countries
count_dict <- container::dict()
for(i in 1:length(regions)){
newDF <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
newDF[freq_var] <- quarters
count_dict[regions[i]] <- newDF
}
##add variables to the dictionary
for( i in 1:length(regions)){
country <- data.frame(row.names = 1:nrow(var_dict[sheetNames[1]]))
country[freq_var] <- quarters
region <- count_dict[regions[i]]
for(j in 1:length(sheetNames)){
datum <- var_dict[sheetNames[j]]
count_names <- colnames(datum)
country[sheetNames[j]]  <- datum[regions[i]]
#var_dict$set(sheetNames[j], xts(var_dict[sheetNames[i]][-1], order.by = as.Date(var_dict[sheetNames[j]][,1], '%Y-%m-%d')))
}
#nuDF <- cbind(region, country)
count_dict$set(regions[i], xts(country[-1], order.by = as.Date(country[,1], '%Y-%m-%d')))
}
print('which year does this data start on ?')
starting_year <- as.numeric(readline('Enter the starting year: '))
##impute missing values:
##Standardize the data
scaled_imp <- container::dict()
for (i in 1:length(regions)){
#convert to ts
scaled_imp[regions[i]] <- ts(count_dict[regions[i]], start = starting_year, frequency = freq_num)
##impute data using seasdec
scaled_imp$set(regions[i], na.seadec(scaled_imp[regions[i]],algorithm = 'interpolation'))
state <- scaled_imp[regions[i]]
scaleDF <- data.frame(row.names = 1:nrow(state))
scaleDF[freq_var] <- as.Date(quarters)
for (j in 1:length(sheetNames)){
vari <- (state[,j] - min(state[,j]))/(max(state[,j]) - min(state[,j]))
scaleDF[sheetNames[j]] <- vari
}
##convert the dataframe to xts then to ts
scaleDF<- xts(scaleDF[,-1], order.by=as.Date(scaleDF[,1], "%Y-%m-%d"))
scaleDF <- ts(scaleDF, start = starting_year, frequency = freq_num)
#scaled_imp$set(regions[i], scaleDF)
}
##convert variable dictionary to xts then standardize
for(i in 1:length(sheetNames)){
var_dict$set(sheetNames[i], xts(var_dict[sheetNames[i]][,-1], order.by = as.Date(var_dict[sheetNames[i]][,1], '%Y-%m-%d')))
country <- var_dict[sheetNames[i]]
##new datafram
nuDF <- data.frame(row.names = 1:nrow(country))
nuDF[freq_var] <- quarters
for(j in 1:length(regions)){
vari <- (country[,j] - min(country[,j]))/(max(country[,j]) - min(country[,j]))
nuDF[regions[j]] <- vari
}
nuDF <- xts(nuDF[,-1], order.by = as.Date(nuDF[,1], '%Y-%m-%d'))
nuDF <- ts(nuDF, start = starting_year, frequency = freq_num)
}
print("IS the data stationary?, Thw following tests whether the data has unit roots or not")
print("If p-value is greater than 0.05 or for augmented dickey, 0.01 - you need to difference to make stationary")
##Augmented Dickey Fuller.
##Test whether it is stationary - constant mean and variace,
##The residuals move around mean of 0
for(i in 1:length(regions)){
print(regions[i])
state <- scaled_imp[regions[i]]
print(augmented_dickey(state))
}
print("Corellation matrix for a country variables")
for(count in 1:length(regions)){
##Things for tomorrow, corellation heatmap
state <- scaled_imp[regions[count]]
print(gg_heatMap(state) + ggtitle(regions[count]))
}
#+ geom_text(aes(Var2, Var1, label=value))
##Stepwise and bestSUbsets for one sheet
print('A function to do an ols and best subsets and print it out')
print('This is time series data, some of the assumptiosn of MLR may be violated, so one should take that into account')
print('Making data stationary is supposed to remove  autocorellation and spurrious regression, sometimes the autocorellation too strong')
##Stepwise
##and ols
check.packages('openxlsx')
wb <- createWorkbook()
print(colnames(scaled_imp[regions[1]]))
y_variable <- readline('Type name of variable to use as Y: ')
for(i in 1:length(regions)){
datum <- scaled_imp[regions[i]]
wb <- OLS_step_best(datum, wb, regions[i],y_variable )
}
saveWorkbook(wb, 'ModelSelection.xlsx')
##remove the package
detach('package:openxlsx', unload = T)
} else if(howManySheet == 'No'){
##convert to zoo file object
xl_data <- read_xlsx(excel_file)
quarters <- as.Date(as.matrix(xl_data[,1]))
xl_data <- as.data.frame(xl_data)
xl_data <- xts(xl_data[,-1], order.by = as.Date(xl_data[,1], "%Y-%m-%d"))
print("Is it Quartely or monthly or annual data")
bool_val <- F
while(bool_val != T){
freq_var <- readline("Enter whether its a 'Quarter', a 'Month' or an 'Annual' data: ")
freq_num <- 0
if(freq_var == 'Quarter'){
freq_num = 4
bool_val = T
} else if(freq_var == 'Month'){
freq_num = 12
bool_val = T
} else if(freq_var == 'Annual'){
freq_num = 1
bool_val = T
}
}
starting_year <- as.numeric(readline("Enter the starting year: "))
xl_data <- ts(xl_data, start = starting_year, frequency = freq_num)
xl_data <- na.seadec(xl_data, algorithm = 'interpolation')
##standardize the data
"Don't standardize the data, it is a bad idea"
scaleDF <- data.frame(row.names = 1:nrow(xl_data))
scaleDF[freq_var] <- as.Date(quarters)
headings_ <- colnames(xl_data)
for(i in 1:length(headings_)){
vari <- (xl_data[,i] - min(xl_data[,i]))/(max(xl_data[,i]) - min(xl_data[,i]))
scaleDF[headings_[i]] <- vari
}
##convert to date and to ts
scaleDF<- xts(scaleDF[,-1], order.by=as.Date(scaleDF[,1], "%Y-%m-%d"))
scaleDF <- ts(scaleDF, start = starting_year, frequency = freq_num)
print("decompose the data")
decomposition(scaleDF)
##check whetehr there are unit roots within the data
##Decide whether you want to make the data stationary afterwards
print(augmented_dickey(scaleDF))
print("is the data stationary??")
##Time series data is rarely stationary and I will make it stationary
print('Make data stationary')
scaleDF <- make_stationary(scaleDF)
##Check if the data is now stationary
print('is the data stationary')
print(augmented_dickey(scaleDF))
print("heatmap of the data")
print(gg_heatMap(scaleDF))
check.packages('openxlsx')
print("OLS function")
wb <- createWorkbook()
print(colnames(scaleDF))
y_variable <- readline('Type name of variable to use as Y: ')
workBook <- OLS_step_best(scaleDF, wb, 'oneSheet', y_variable)
file_out = T
while(file_out == T){
fileToSave <- readline('Save your file as: ')
fileToSave <- paste(fileToSave, 'xlsx',sep = '.')
if(file.exists(fileToSave) == T){
print('Filename already exists')
} else{
file_out = F
}
}
saveWorkbook(workBook, fileToSave)
detach('package:openxlsx', unload = T)
##heatmap of alll all all all all alla all a
##heatmap(xx, Rowv = F, symm = T, distfun = function(c) as.dist(1 - c), hclustfun = function(d)hclust(d, method = 'single'), keep.dendro = F)
}
View(datum)
glsModel <- function(data_frame, variables){
check.packages('nlme')
"data <- data.frame(row.names = 1:nrow(data_frame))
colNames <- colnames(data_frame)
for(i in 1:variables){
data[colNames[i]] <- data[,i]
}"
model <- gls(data_frame[,1] ~ data_frame[,2:ncol(datum)])
return(model)
}
glsModel <- function(data_frame){
check.packages('nlme')
"data <- data.frame(row.names = 1:nrow(data_frame))
colNames <- colnames(data_frame)
for(i in 1:variables){
data[colNames[i]] <- data[,i]
}"
model <- gls(data_frame[,1] ~ data_frame[,2:ncol(datum)])
return(model)
}
glsModel(datum)
glsModel <- function(data_frame){
check.packages('nlme')
"data <- data.frame(row.names = 1:nrow(data_frame))
colNames <- colnames(data_frame)
for(i in 1:variables){
data[colNames[i]] <- data[,i]
}"
model <- gls(data_frame[,1] ~ data_frame[,2:ncol(data_frame)])
return(model)
}
glsModel(datum)
modelfn <- function(data_frame){
modelfit <- lm(data_frame[,1]~data_frame[,2:ncol(data_frame)])
fn = step(modelfit)
return(step)
}
modelfn(datum)
modelfn <- function(data_frame){
modelfit <- lm(data_frame[,1]~data_frame[,2:ncol(data_frame)])
fn = step(modelfit)
return(modelfit)
}
modelfn(datum)
View(datum)
diff(datum)
View(diff(datum))
modelfndiff <- function(data_frame){
differenced <- diff(data_frame)
modeldiff <- lm(differenced[,1] ~ differenced[,2:ncol(differenced)])
}
judas modelfndiff(datum)
judas =modelfndiff(datum)
judas$coefficients
judas = glsModel(datum)
judas$coefficients
judas$coefs
judas$coeffs
judas$coefficients
judas =modelfndiff(datum)
judas$coefs
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
summary(glsMod)
judas = glsModel(datum)
summary(judas)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
source('~/Documents/futuregrowth_research/codebase/fairValue.R', echo=TRUE)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
judas = modelfn(datum)
judas = summary(modelfn(datum))
judas$coefs
judas$coefficients
View(judas$coefficients)
judas = summary(glsModel(datum))
View(judas$coefficients)
source('~/Documents/futuregrowth_research/thabo_environment/Emerging_Markets_Research.R', echo=TRUE)
